{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&P500 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_time = int(time.time())\n",
    "#prior_year = curr_time-(60*60*24*365)\n",
    "prior_year=1543622400\n",
    "ticker_symbol = \"^GSPC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://apidojo-yahoo-finance-v1.p.rapidapi.com/stock/get-histories\"\n",
    "\n",
    "querystring = {\"region\":\"US\",\"lang\":\"en\",\"symbol\":ticker_symbol,\"from\":prior_year,\"to\":curr_time,\"events\":\"div\",\"events\":\"split\",\"events\":\"earn\",\"interval\":\"1d\"}\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-host': \"apidojo-yahoo-finance-v1.p.rapidapi.com\",\n",
    "    'x-rapidapi-key': \"c9571a29b7msh7dedf518d9ba8b2p1bf4b5jsnec4ec11b7588\"\n",
    "    }\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = response.json()['chart']['result'][0]['timestamp']\n",
    "year_close = response.json()['chart']['result'][0]['indicators']['quote'][0]['close']\n",
    "year_open =response.json()['chart']['result'][0]['indicators']['quote'][0]['open']\n",
    "year_volume =response.json()['chart']['result'][0]['indicators']['quote'][0]['volume']\n",
    "year_adjclose =response.json()['chart']['result'][0]['indicators']['adjclose'][0]['adjclose']\n",
    "\n",
    "    \n",
    "snpDict = {\"Timestamp\":timestamp,\n",
    "           \"Open\":year_open,\n",
    "          \"Close\":year_close,\n",
    "           \"Volume\":year_volume,\n",
    "          \"Adjclose\":year_adjclose\n",
    "          }\n",
    "snp_df = pd.DataFrame(snpDict)\n",
    "\n",
    "snp_df['Change %'] = ((snp_df['Close']-snp_df['Open'])/snp_df['Open'])*100\n",
    "\n",
    "convert_date = []\n",
    "convert_date = [datetime.datetime.utcfromtimestamp(snp_df['Timestamp'][x]).strftime('%Y-%m-%d') for x in range(0,len(snp_df['Timestamp']))]\n",
    "snp_df['Date'] = convert_date\n",
    "\n",
    "\n",
    "snp_df['Last Year'] =[datetime.datetime.strptime(snp_df['Date'][x],'%Y-%m-%d')-datetime.timedelta(365) for x in range(0,len(snp_df['Date']))]\n",
    "snp_df2=snp_df.sort_values(by='Date',ascending=False)\n",
    "snp_df2=snp_df.head(59)\n",
    "snp_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volatile_yr_avg = []\n",
    "yearlyGroups = []\n",
    "yearlyTimestamp = []\n",
    "\n",
    "for i in range(0,len(snp_df['Timestamp'])):\n",
    "    try:\n",
    "        cond1 = snp_df['Timestamp']<=snp_df['Timestamp'][i]\n",
    "        cond2 = (pd.Series(snp_df['Timestamp']>=(snp_df['Timestamp'][i]-(365*24*60*60))))\n",
    "        df = snp_df[cond1 & cond2]\n",
    "        #print('working')\n",
    "        if len(df)>251:  #avoids sets less than a full year\n",
    "            volatile_yr_avg.append(df.std()['Change %']*math.sqrt(len(df))*100) #annual volatility (stdev * sqrt(252))\n",
    "            yearlyGroups.append(snp_df['Date'][i]) \n",
    "            yearlyTimestamp.append(snp_df['Timestamp'][i])\n",
    "            \n",
    "    except:\n",
    "        print('Error Found')\n",
    "volatile_snp_dict = {\"Date\":yearlyGroups,\n",
    "                    \"Annual Volatility\":volatile_yr_avg,\n",
    "                     \"Timestamp\":yearlyTimestamp\n",
    "                    }\n",
    "volatile_snp_df = pd.DataFrame(volatile_snp_dict)\n",
    "volatile_snp_df['Date']=pd.to_datetime(volatile_snp_df['Date'])\n",
    "snp_volatility=volatile_snp_df.sort_values(by='Date',ascending=False)\n",
    "snp_volatility=snp_volatility.head(59)\n",
    "\n",
    "snp_volatility.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DJI Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_time = int(time.time())\n",
    "ticker_symbol = \"^DJI\"\n",
    "prior_year=1543622400\n",
    "url = \"https://apidojo-yahoo-finance-v1.p.rapidapi.com/stock/get-histories\"\n",
    "\n",
    "querystring = {\"region\":\"US\",\"lang\":\"en\",\"symbol\":ticker_symbol,\"from\":prior_year,\"to\":curr_time,\"events\":\"div\",\"events\":\"split\",\"events\":\"earn\",\"interval\":\"1d\"}\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-host': \"apidojo-yahoo-finance-v1.p.rapidapi.com\",\n",
    "    'x-rapidapi-key': \"c9571a29b7msh7dedf518d9ba8b2p1bf4b5jsnec4ec11b7588\"\n",
    "    }\n",
    "\n",
    "response_DJI = requests.request(\"GET\", url, headers=headers, params=querystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = response_DJI.json()['chart']['result'][0]['timestamp']\n",
    "year_close = response_DJI.json()['chart']['result'][0]['indicators']['quote'][0]['close']\n",
    "year_open =response_DJI.json()['chart']['result'][0]['indicators']['quote'][0]['open']\n",
    "year_volume =response_DJI.json()['chart']['result'][0]['indicators']['quote'][0]['volume']\n",
    "year_adjclose =response_DJI.json()['chart']['result'][0]['indicators']['adjclose'][0]['adjclose']\n",
    "\n",
    "dji_Dict = {\"Timestamp\":timestamp,\n",
    "           \"Open\":year_open,\n",
    "          \"Close\":year_close,\n",
    "           \"Volume\":year_volume,\n",
    "          \"Adjclose\":year_adjclose\n",
    "          }\n",
    "dji_df = pd.DataFrame(dji_Dict)\n",
    "\n",
    "dji_df['Change %'] = ((dji_df['Close']-dji_df['Open'])/dji_df['Open'])*100\n",
    "\n",
    "convert_date = []\n",
    "convert_date = [datetime.datetime.utcfromtimestamp(dji_df['Timestamp'][x]).strftime('%Y-%m-%d') for x in range(0,len(dji_df['Timestamp']))]\n",
    "dji_df['Date'] = convert_date\n",
    "\n",
    "\n",
    "dji_df['Last Year'] =[datetime.datetime.strptime(dji_df['Date'][x],'%Y-%m-%d')-datetime.timedelta(365) for x in range(0,len(dji_df['Date']))]\n",
    "dji_df['Date']=pd.to_datetime(dji_df['Date'],errors='coerce')\n",
    "\n",
    "dji_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volatile_yr_avg = []\n",
    "yearlyGroups = []\n",
    "yearlyTimestamp = []\n",
    "\n",
    "for i in range(0,len(dji_df['Timestamp'])):\n",
    "    try:\n",
    "        cond1 = dji_df['Timestamp']<=dji_df['Timestamp'][i]\n",
    "        cond2 = (pd.Series(dji_df['Timestamp']>=(dji_df['Timestamp'][i]-(365*24*60*60))))\n",
    "        df = dji_df[cond1 & cond2]\n",
    "        #print('working')\n",
    "        if len(df)>251:  #avoids sets less than a full year\n",
    "            volatile_yr_avg.append(df.std()['Change %']*math.sqrt(len(df))*100) #annual volatility (stdev * sqrt(252))\n",
    "            yearlyGroups.append(dji_df['Date'][i]) \n",
    "            yearlyTimestamp.append(dji_df['Timestamp'][i])\n",
    "            \n",
    "    except:\n",
    "        print('Error Found')\n",
    "volatile_dji_dict = {\"Date\":yearlyGroups,\n",
    "                    \"Annual Volatility\":volatile_yr_avg,\n",
    "                     \"Timestamp\":yearlyTimestamp\n",
    "                    }\n",
    "volatile_dji_df = pd.DataFrame(volatile_dji_dict)\n",
    "volatile_dji_df['Date']=pd.to_datetime(volatile_dji_df['Date'])\n",
    "volatile_dji_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shanghei Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_symbol = \"000001.SS\"\n",
    "url = \"https://apidojo-yahoo-finance-v1.p.rapidapi.com/stock/get-histories\"\n",
    "\n",
    "querystring = {\"region\":\"US\",\"lang\":\"en\",\"symbol\":ticker_symbol,\"from\":prior_year,\"to\":curr_time,\"events\":\"div\",\"events\":\"split\",\"events\":\"earn\",\"interval\":\"1d\"}\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-host': \"apidojo-yahoo-finance-v1.p.rapidapi.com\",\n",
    "    'x-rapidapi-key': \"c9571a29b7msh7dedf518d9ba8b2p1bf4b5jsnec4ec11b7588\"\n",
    "    }\n",
    "\n",
    "response_ssec = requests.request(\"GET\", url, headers=headers, params=querystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = response_ssec.json()['chart']['result'][0]['timestamp']\n",
    "year_close = response_ssec.json()['chart']['result'][0]['indicators']['quote'][0]['close']\n",
    "year_open =response_ssec.json()['chart']['result'][0]['indicators']['quote'][0]['open']\n",
    "year_volume =response_ssec.json()['chart']['result'][0]['indicators']['quote'][0]['volume']\n",
    "year_adjclose =response_ssec.json()['chart']['result'][0]['indicators']['adjclose'][0]['adjclose']\n",
    "\n",
    "ssec_Dict = {\"Timestamp\":timestamp,\n",
    "           \"Open\":year_open,\n",
    "          \"Close\":year_close,\n",
    "           \"Volume\":year_volume,\n",
    "          \"Adjclose\":year_adjclose\n",
    "          }\n",
    "ssec_df = pd.DataFrame(ssec_Dict)\n",
    "\n",
    "ssec_df['Change %'] = ((ssec_df['Close']-ssec_df['Open'])/ssec_df['Open'])*100\n",
    "\n",
    "convert_date = []\n",
    "convert_date = [datetime.datetime.utcfromtimestamp(ssec_df['Timestamp'][x]).strftime('%Y-%m-%d') for x in range(0,len(ssec_df['Timestamp']))]\n",
    "ssec_df['Date'] = convert_date\n",
    "\n",
    "\n",
    "ssec_df['Last Year'] =[datetime.datetime.strptime(ssec_df['Date'][x],'%Y-%m-%d')-datetime.timedelta(365) for x in range(0,len(ssec_df['Date']))]\n",
    "\n",
    "ssec_df2=ssec_df.sort_values(by='Date',ascending=False)\n",
    "ssec_df2=ssec_df.head(59)\n",
    "ssec_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volatile_yr_avg = []\n",
    "yearlyGroups = []\n",
    "yearlyTimestamp = []\n",
    "\n",
    "for i in range(0,len(ssec_df['Timestamp'])):\n",
    "    try:\n",
    "        cond1 = ssec_df['Timestamp']<=ssec_df['Timestamp'][i]\n",
    "        cond2 = (pd.Series(ssec_df['Timestamp']>=(ssec_df['Timestamp'][i]-(365*24*60*60))))\n",
    "        df = ssec_df[cond1 & cond2]\n",
    "        #print('working')\n",
    "        if len(df)>237:  #avoids sets less than a full year\n",
    "            volatile_yr_avg.append(df.std()['Change %']*math.sqrt(len(df))*100) #annual volatility (stdev * sqrt(252))\n",
    "            yearlyGroups.append(ssec_df['Date'][i]) \n",
    "            yearlyTimestamp.append(ssec_df['Timestamp'][i])\n",
    "            \n",
    "    except:\n",
    "        print('Error Found')\n",
    "volatile_ssec_dict = {\"Date\":yearlyGroups,\n",
    "                    \"Annual Volatility\":volatile_yr_avg,\n",
    "                     \"Timestamp\":yearlyTimestamp\n",
    "                    }\n",
    "volatile_ssec_df = pd.DataFrame(volatile_ssec_dict)\n",
    "volatile_ssec_df['Date']=pd.to_datetime(volatile_ssec_df['Date'])\n",
    "ssec_volatility=volatile_ssec_df.sort_values(by='Date',ascending=False)\n",
    "ssec_volatility=ssec_volatility.head(59)\n",
    "ssec_volatility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_df=pd.read_csv(\"italy_index/FTSE Italia All Share Historical Data.csv\")\n",
    "italy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries=italy_df['Price']\n",
    "print(\"p-value:\",adfuller(timeseries.dropna())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ARIMA(timeseries,order=(1,1,0))\n",
    "results=model.fit()\n",
    "results.plot_predict(1,365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_fut=italy_df.drop(['Open','High','Low','Vol.','Change %','Timestamp'],axis=1)\n",
    "italy_fut['Date']=pd.DatetimeIndex(italy_fut['Date'])\n",
    "history=italy_fut['Price']\n",
    "model=ARIMA(history,order=(5,1,0))\n",
    "model_fit=model.fit()\n",
    "model_fit.plot_predict(1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_2019_ncov_Deaths =pd.read_csv(\"COVID19_data/time_series_2019-ncov-Deaths.csv\")\n",
    "time_series_2019_ncov_Recovered = pd.read_csv(\"COVID19_data/time_series_2019-ncov-Recovered.csv\")\n",
    "time_series_2019_ncov_Confirmed = pd.read_csv(\"COVID19_data/time_series_2019-ncov-Confirmed.csv\")\n",
    "time_series_ncov_Recovered = pd.read_csv(\"COVID19_data/time_series-ncov-Recovered.csv\")\n",
    "time_series_ncov_Confirmed = pd.read_csv(\"COVID19_data/time_series-ncov-Confirmed.csv\")\n",
    "time_series_ncov_Deaths = pd.read_csv(\"COVID19_data/time_series-ncov-Deaths.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unneeded columns\n",
    "time_series_ncov_Confirmed =time_series_ncov_Confirmed.drop(columns =['Province/State'])\n",
    "time_series_ncov_Deaths =time_series_ncov_Deaths.drop(columns =['Province/State'])\n",
    "time_series_ncov_Recovered =time_series_ncov_Recovered.drop(columns =['Province/State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unecessary rows in each data set\n",
    "\n",
    "time_series_ncov_Confirmed =time_series_ncov_Confirmed.drop([0], axis = 0)\n",
    "time_series_ncov_Deaths =time_series_ncov_Deaths.drop([0], axis = 0)\n",
    "time_series_ncov_Recovered =time_series_ncov_Recovered.drop([0], axis = 0)\n",
    "time_series_ncov_Deaths.head()\n",
    "\n",
    "\n",
    "\n",
    "# Changing all value columns to numeric for analysis\n",
    "time_series_ncov_Confirmed[\"Value\"] = pd.to_numeric(time_series_ncov_Confirmed[\"Value\"])\n",
    "time_series_ncov_Deaths[\"Value\"] = pd.to_numeric(time_series_ncov_Deaths[\"Value\"])\n",
    "time_series_ncov_Recovered[\"Value\"] = pd.to_numeric(time_series_ncov_Recovered[\"Value\"])\n",
    "\n",
    "\n",
    "time_series_ncov_Confirmed[\"Date\"] = pd.to_datetime(time_series_ncov_Confirmed[\"Date\"])\n",
    "time_series_ncov_Deaths[\"Date\"] = pd.to_datetime(time_series_ncov_Deaths[\"Date\"])\n",
    "time_series_ncov_Recovered[\"Date\"] = pd.to_datetime(time_series_ncov_Recovered[\"Date\"])\n",
    "\n",
    "time_series_ncov_Confirmed.dtypes\n",
    "\n",
    "#Renaming value columns to be easier to analyze\n",
    "\n",
    "time_series_ncov_Confirmed= time_series_ncov_Confirmed.rename(columns ={'Value':'Confirmed Cases'})\n",
    "time_series_ncov_Deaths= time_series_ncov_Deaths.rename(columns ={'Value':'Deaths'})\n",
    "time_series_ncov_Recovered= time_series_ncov_Recovered.rename(columns ={'Value':'Recoveries'})\n",
    "time_series_ncov_Confirmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deaths-China & US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_cond=time_series_2019_ncov_Deaths['Country/Region']=='China'\n",
    "\n",
    "china_deaths = time_series_2019_ncov_Deaths[china_cond]\n",
    "china_deaths =china_deaths.drop(columns =['Lat','Long','Province/State'])\n",
    "china_deaths = china_deaths.groupby('Country/Region').sum()\n",
    "\n",
    "china_deaths =china_deaths.T\n",
    "china_deaths.index = pd.to_datetime(china_deaths.index)\n",
    "#china_deaths.plot(legend=False)\n",
    "\n",
    "china_daily = [0]\n",
    "china_daily_percent = [0]\n",
    "for i in range(1,len(china_deaths['China'])):\n",
    "    china_daily.append(china_deaths['China'][i]-china_deaths['China'][i-1])\n",
    "    if china_deaths['China'][i-1]>0:\n",
    "        china_daily_percent.append((china_deaths['China'][i]-china_deaths['China'][i-1])/china_deaths['China'][i-1])\n",
    "    else:\n",
    "        china_daily_percent.append(0)    \n",
    "    \n",
    "    \n",
    "china_daily_df = china_deaths[['China']]\n",
    "china_daily_df['Daily']=china_daily\n",
    "china_daily_df['Percent Change'] = china_daily_percent\n",
    "\n",
    "#china_daily_df.plot(legend=False)\n",
    "china_daily_df['Percent Change'].plot(legend=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cond=time_series_2019_ncov_Deaths['Country/Region']=='US'\n",
    "\n",
    "us_deaths = time_series_2019_ncov_Deaths[us_cond]\n",
    "us_deaths =us_deaths.drop(columns =['Lat','Long','Province/State'])\n",
    "us_deaths = us_deaths.groupby('Country/Region').sum()\n",
    "\n",
    "us_deaths =us_deaths.T\n",
    "us_deaths.index = pd.to_datetime(us_deaths.index)\n",
    "#us_deaths.plot(legend=False)\n",
    "\n",
    "us_daily = [0]\n",
    "us_daily_percent = [0]\n",
    "for i in range(1,len(us_deaths['US'])):\n",
    "    us_daily.append(us_deaths['US'][i]-us_deaths['US'][i-1])\n",
    "    if us_deaths['US'][i-1]>0:\n",
    "        us_daily_percent.append((us_deaths['US'][i]-us_deaths['US'][i-1])/us_deaths['US'][i-1])\n",
    "    else:\n",
    "        us_daily_percent.append(0)\n",
    "us_daily_df = us_deaths[['US']]\n",
    "us_daily_df['Daily']=us_daily\n",
    "us_daily_df['Percent Change'] = us_daily_percent\n",
    "\n",
    "#us_daily_df.plot(legend=False)\n",
    "us_daily_df['Percent Change'].plot(legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recovered Cases-China & US\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cond=time_series_2019_ncov_Recovered['Country/Region']=='US'\n",
    "\n",
    "us_recovered = time_series_2019_ncov_Recovered[us_cond]\n",
    "us_recovered =us_recovered.drop(columns =['Lat','Long','Province/State'])\n",
    "us_recovered = us_recovered.groupby('Country/Region').sum()\n",
    "\n",
    "us_recovered =us_recovered.T\n",
    "us_recovered.index = pd.to_datetime(us_recovered.index)\n",
    "#us_recovered.plot(legend=False)\n",
    "\n",
    "us_daily = [0]\n",
    "us_daily_percent = [0]\n",
    "for i in range(1,len(us_recovered['US'])):\n",
    "    \n",
    "    us_daily.append(us_recovered['US'][i]-us_recovered['US'][i-1])\n",
    "    if us_recovered['US'][i-1]>0:\n",
    "        us_daily_percent.append((us_recovered['US'][i]-us_recovered['US'][i-1])/us_recovered['US'][i-1])\n",
    "    else:\n",
    "        us_daily_percent.append(0)\n",
    "us_daily_df = us_recovered[['US']]\n",
    "us_daily_df['Daily']=us_daily\n",
    "us_daily_df['Percent Change'] = us_daily_percent\n",
    "\n",
    "#us_daily_df.plot(legend=False)\n",
    "us_daily_df['Percent Change'].plot(legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_cond=time_series_2019_ncov_Recovered['Country/Region']=='China'\n",
    "\n",
    "china_recovered = time_series_2019_ncov_Recovered[china_cond]\n",
    "china_recovered =china_recovered.drop(columns =['Lat','Long','Province/State'])\n",
    "china_recovered = china_recovered.groupby('Country/Region').sum()\n",
    "\n",
    "china_recovered =china_recovered.T\n",
    "china_recovered.index = pd.to_datetime(china_recovered.index)\n",
    "#china_recovered.plot(legend=False)\n",
    "\n",
    "china_daily = [0]\n",
    "china_daily_percent = [0]\n",
    "for i in range(1,len(china_recovered['China'])):\n",
    "    china_daily.append(china_recovered['China'][i]-china_recovered['China'][i-1])\n",
    "    if china_recovered['China'][i-1]>0:\n",
    "        china_daily_percent.append((china_recovered['China'][i]-china_recovered['China'][i-1])/china_recovered['China'][i-1])\n",
    "    else:\n",
    "        china_daily_percent.append(0)\n",
    "        \n",
    "china_daily_df = china_recovered[['China']]\n",
    "china_daily_df['Daily']=china_daily\n",
    "china_daily_df['Percent Change'] = china_daily_percent\n",
    "\n",
    "china_daily_df.columns=['Total Cases','Daily Cases','Percent Change']\n",
    "#china_daily_df.plot(legend=False)\n",
    "china_daily_df['Percent Change'].plot(legend=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirmed Cases-China & US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cond=time_series_2019_ncov_Confirmed['Country/Region']=='US'\n",
    "\n",
    "us_confirmed = time_series_2019_ncov_Confirmed[us_cond]\n",
    "us_confirmed =us_confirmed.drop(columns =['Lat','Long','Province/State'])\n",
    "us_confirmed = us_confirmed.groupby('Country/Region').sum()\n",
    "\n",
    "us_confirmed =us_confirmed.T\n",
    "us_confirmed.index = pd.to_datetime(us_confirmed.index)\n",
    "#us_confirmed.plot(legend=True,label = \"Total Cases\")\n",
    "\n",
    "us_daily = [0]\n",
    "us_daily_percent = [0]\n",
    "for i in range(1,len(us_confirmed['US'])):\n",
    "    us_daily.append(us_confirmed['US'][i]-us_confirmed['US'][i-1])\n",
    "    if us_confirmed['US'][i-1]>0:\n",
    "        us_daily_percent.append((us_confirmed['US'][i]-us_confirmed['US'][i-1])/us_confirmed['US'][i-1])\n",
    "    else:\n",
    "        us_daily_percent.append(0)\n",
    "us_daily_df = us_confirmed[['US']]\n",
    "us_daily_df['Daily']=us_daily\n",
    "us_daily_df['Percent Change'] = us_daily_percent\n",
    "\n",
    "us_daily_plot = us_daily_df['Daily']\n",
    "#us_daily_plot.plot(legend=False,label = \"Daily Cases\")\n",
    "us_daily_df['Percent Change'].plot(legend=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_cond=time_series_2019_ncov_Confirmed['Country/Region']=='China'\n",
    "\n",
    "china_confirmed = time_series_2019_ncov_Confirmed[china_cond]\n",
    "china_confirmed =china_confirmed.drop(columns =['Lat','Long','Province/State'])\n",
    "china_confirmed = china_confirmed.groupby('Country/Region').sum()\n",
    "\n",
    "china_confirmed =china_confirmed.T\n",
    "china_confirmed.index = pd.to_datetime(china_confirmed.index)\n",
    "#china_confirmed.plot(legend=False)\n",
    "\n",
    "china_daily = [0]\n",
    "china_daily_percent = [0]\n",
    "for i in range(1,len(china_confirmed['China'])):\n",
    "    china_daily.append(china_confirmed['China'][i]-china_confirmed['China'][i-1])\n",
    "    if china_confirmed['China'][i-1]>0:\n",
    "        china_daily_percent.append((china_confirmed['China'][i]-china_confirmed['China'][i-1])/china_confirmed['China'][i-1])\n",
    "    else:\n",
    "        china_daily_percent.append(0)\n",
    "        \n",
    "china_daily_df = china_confirmed[['China']]\n",
    "china_daily_df['Daily']=china_daily\n",
    "china_daily_df['Percent Change'] = china_daily_percent\n",
    "china_daily_plot = china_daily_df['Daily']\n",
    "#china_daily_plot.plot(legend=False)\n",
    "china_daily_df['Percent Change'].plot(legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Italy_cond=time_series_2019_ncov_Confirmed['Country/Region']=='Italy'\n",
    "\n",
    "Italy_confirmed = time_series_2019_ncov_Confirmed[Italy_cond]\n",
    "Italy_confirmed =Italy_confirmed.drop(columns =['Lat','Long','Province/State'])\n",
    "Italy_confirmed = Italy_confirmed.groupby('Country/Region').sum()\n",
    "\n",
    "Italy_confirmed =Italy_confirmed.T\n",
    "Italy_confirmed.index = pd.to_datetime(Italy_confirmed.index)\n",
    "#china_confirmed.plot(legend=False)\n",
    "\n",
    "Italy_daily = [0]\n",
    "Italy_daily_percent = [0]\n",
    "for i in range(1,len(Italy_confirmed['Italy'])):\n",
    "    Italy_daily.append(Italy_confirmed['Italy'][i]-Italy_confirmed['Italy'][i-1])\n",
    "    if Italy_confirmed['Italy'][i-1]>0:\n",
    "        Italy_daily_percent.append((Italy_confirmed['Italy'][i]-Italy_confirmed['Italy'][i-1])/Italy_confirmed['Italy'][i-1])\n",
    "    else:\n",
    "        Italy_daily_percent.append(0)\n",
    "        \n",
    "Italy_daily_df = Italy_confirmed[['Italy']]\n",
    "Italy_daily_df['Daily']=Italy_daily\n",
    "Italy_daily_df['Percent Change'] = Italy_daily_percent\n",
    "Italy_daily_plot = Italy_daily_df['Daily']\n",
    "#china_daily_plot.plot(legend=False)\n",
    "Italy_daily_df['Percent Change'].plot(legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(china_daily_df['Percent Change'],ssec_volatility['Annual Volatility'])\n",
    "plt.xlabel('Confirm Cases % Change')\n",
    "plt.ylabel('Annual Volatility')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(china_daily_df['Percent Change'],ssec_df2['Change %'])\n",
    "plt.xlabel('Confirm Cases % Change')\n",
    "plt.ylabel('Daily % Change')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(us_daily_df['Percent Change'],snp_df2['Change %'])\n",
    "plt.ylabel('Daily % Change')\n",
    "plt.xlabel('Confirm Cases % Change')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_fut=snp_df.drop(['Open','Adjclose','Volume','Close','Timestamp','Last Year'],axis=1)\n",
    "snp_fut['Date']=pd.DatetimeIndex(snp_fut['Date'])\n",
    "snp_fut=snp_fut.rename(columns={'Date': 'ds',\n",
    "                                    'Change %': 'Change %'})\n",
    "\n",
    "# ax2=ax1.twinx()\n",
    "# ax2=china_daily_df['Percent Change'].plot(figsize=(10,10))\n",
    "\n",
    "ax1=snp_fut.set_index('ds').plot(figsize=(10,10))\n",
    "ax1.set_ylabel('Daily % Change')\n",
    "ax1.set_xlabel('Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Italy_daily_df.reset_index()['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Italy_daily_df.reset_index()['index'],Italy_daily_df.reset_index()['Percent Change'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dji_df[\"Date\"]=pd.to_datetime(dji_df[\"Date\"])\n",
    "#stock price vs confirmed cases\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Stock Price')\n",
    "ax1.plot(italy_df['Date'],italy_df['Price'], label = \"Stock Price\", color=color)\n",
    "#plt.xlim(datetime.date(2020,1,1),datetime.date(2020,3,26))\n",
    "#plt.yscale('symlog')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "ax2 = ax1.twinx()\n",
    "color1 = 'tab:blue'\n",
    "color2 = 'tab:green'\n",
    "color3 = 'tab:orange'\n",
    "ax2.set_ylabel('Daily % Change')\n",
    "ax2.scatter(Italy_daily_df.reset_index()['index'],Italy_daily_df.reset_index()['Percent Change'])\n",
    "ax2.scatter(us_daily_df.reset_index()['index'],us_daily_df.reset_index()['Percent Change'])\n",
    "ax2.scatter(china_daily_df.reset_index()['index'],china_daily_df.reset_index()['Percent Change'])\n",
    "\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(us_confirmed['US'], label = \"Confirmed Cases\")\n",
    "ax1.plot(us_recovered['US'],label = \"Recovered\")\n",
    "ax1.plot(us_deaths['US'], label =\"Deaths\")\n",
    "plt.yscale('symlog')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xticks(rotation=90)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(volatile_snp_df['Date'],volatile_snp_df['Annual Volatility'], label = \"Volatility\")\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "ax1.plot(china_confirmed['China'], label = \"Confirmed Cases\")\n",
    "ax1.plot(china_recovered['China'],label = \"Recovered\")\n",
    "ax1.plot(china_deaths['China'],label = \"Deaths\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=90)\n",
    "plt.yscale('symlog')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(volatile_ssec_df['Date'],volatile_ssec_df['Annual Volatility'],label = \"Volatility\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
